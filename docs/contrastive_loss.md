# Loss function

## Maximum Likelihood Estimation (MLE)

We first derive the liklihood of a single event at time $t_i$  with observation $X = \{(\Delta t_i,W_i)\}$ and history-embdded context vector $C_i$ (see above/below). The observation is embedded as $Z_i = \epsilon(W_i)$ where $\epsilon$ is an arbitrary dimesion-reducing embedding network (see above/below). As we will see, during training this embedded space will be organized such that waveform features that occur in similar contexts will group together within this space. [TODO: make that sentence better]

 Qualitatively, the likelihood of this observation is the probability of observing a mark with embedded waveform features $Z_i$ at a delay $\Delta t_i$ with no other marks occuring within that interval. This is defined by the equation:

 $$ p_i \equiv p(Z_i, \Delta t_i | C_i-1) =  H_i (e^ {\int_Z ln(S_j) dj }) = H_i \bar S$$

where $H_j \equiv H(Z_j, \Delta t_i |C_{i-1})$ is the context-dependent hazard function for a given embedded mark and delay, and  $S_j \equiv S(Z_j, \Delta t_i |C_{i-1})$ is the context-dependent survival function for the same. $\bar S$ is the cumulative survival function over all possible marks and behaves similar to a partition function.

To estimate a likelihood for model training, we can approximate the cumulative survival function by sampling embedded marks from other points in the data.:
$$\bar S \approx S_i \prod_{j\in{negative\ samples}}S_j$$
Combining this with equation (xxx) above gives us a general negative log liklihood we can use for trainging loss:

$$\mathcal{L}_i = - \log H_i - \sum_{j\in{\set {i, negative\ samples}}} \log S_j $$

We note that under a Poisson process model such that $H_j = H(Z_j, \Delta t_i, C_{i-1}) = \lambda(Z_j,C_{i-1}) \equiv \lambda_j$ this loss reduces to:

$$\mathcal{L}_i = -\log \lambda_i + \Delta t_i \sum_{j\in{\set {i, negative\ samples}}} \lambda_j$$

In the case of enumerable ordinal mark features where $\bar S$ can be calculated exactly, this equation is  equivalent to the MLE loss presented in other works on marked point processes [CITE].

## Contrastive Loss

In our case where $Z$ is a continuous feature vector, the MLE approach is limited by our ability to estimate $\bar S$. This is because 1. in high-density neural data there are up to thousands of emission sources to sample and 2. sampling of marks across Z will be biased towards high-firing rate regions of the space.  Instead we derive an alternative loss function using Noise Contrastive Estimation (NCE) [CITE].

Consider a collection of marks including both the true value $Z_i$ and $m$ false events from elsewhere in the data to make a 'bag' of samples:
$$ M = \set{Z_i} \cup\set{Z_j}_{j=1}^m$$

Each of these marks has a probability of occuring under the true model $p_j = H_j\bar S$ (eqn. xxx). We can also define an alternative 'noise' model for mark generation, and corresponding probability of each event occuring under this alternative model:
$$q_j \equiv q(Z_j, \Delta t_i, C_{i-1}) =  H'_i (e^ {\int_Z ln(S'_j) dj }) = H'_i \bar S'$$

Where $H'$ and $S'$ are the conditional hazard and survival functions of the alternative model respectively.

To optimize our embedding we pose the question: Given that exactly one $Z_j \in M$ was generated by the true model and the rest by the alternative model; what is the probability that the true value $Z_i$ was generated by the true model ($p(Z_i = true)$)?

The relative probibility that a given mark $Z_j$ from our bag is the one from the true prosses is $p_j \prod_{k\neq j}q_k$. Given that, the probability we seek to optimize is:

$$p(Z_i = true) = \frac{p_i\prod_{k\neq i}q_k}{\sum_j(p_j \prod_{k\neq j}q_k)} = \frac{p_i/qi}{\sum_jp_j/q_j}$$

Plugging in our equations for the probability under each model from before, we get:

$$p(Z_i=true) = \frac{H_i\bar S / (H'_i\bar S')}{\sum_jH_j\bar S / (H'_j\bar S')} = \frac{H_i/H'_i}{\sum_j H_j/H'_j}$$

Our loss for training is then:
$$\mathcal{L} = -\log p(Z_i = true) = -\log(H_i/H'_i) + \log\sum_j(H_j/H'_j)$$

Beacuse all samples share the same probabilistic dependence on the lack of prior events ($\bar S$) this term becomes irrelevant to the contrastive question. This is critical as we no longer need to (poorly) approximate it across our continuous mark space $Z$. In this case, increasing the number of negative samples in $M$ increases the _difficulty_ of identifying the true sample $Z_i$ (and can therfore help to refine the model), but the loss equation is made no more or less _accurate_ by doing so.

Similarly, as can be seen in eqn. xxx, any multiplicative factor in $H$ or $H'$ that is constant with respect to $Z$ is negated within the loss. This is relevant to bear in mind when designing hazard models as we will see below.

## Alternative Model Selection ($H'$)

### Uniform intensity

We first consider the case wher the hazard function has no dependence on the embedded waveform such that $H'(Z_j, \Delta t_i | C_{i-1}) = H'(\Delta t_i | C_{i-1})$

As mentioned above, any constant factor in the hazard function can be safely ignored, meaning we can specify $H'(\Delta t_i | C_{i-1}) = 1$ with no loss of generality. This results in the loss function:
$$\mathcal{L} = -\log p(Z_i = true) = -\log H_i + \log\sum_jH_j$$

While this case does not make the best use of the model expressivity, we mention it to highlite an interesting equivalency.  We take the additional assumption that the true hazard model is indepent of the delay time ($H(Z_j,\Delta t |C_{i-1}) = H(Z_j |C_{i-1})$). This case can occur if assuming a Poisson emission model _or_ if all marks are evenly spaced in time (therfore making $\Delta t$) a constant rather than a variable).

The later matches the modeling conditions used for Contrastive Predictive Coding (CPC) for unsupervised representation learning of a resularly sampled timeseries [Oord et. al 2019]. Accordingly, replacing $H(Z_j | C_{i-1})$ with $f(Z_j | C_{i-1})$ exactly reproduces the loss function dervied in the paper (cited eqn. 4).

## Regularization of the mark space

We next explore alternative models that help to structure the learned representations. In particular we take the case where the alternative model is poisson process independent of the current context, where the intensity decays with the k-norm of the embedded waveform vector:

$$H'_j = e^{-(||Z_j||_{k})^k}$$

Intuitively, this alternative model serves to regularize the embedded space during the training process. For a given embedded mark $Z_j$ on timepoints {$i\neq j$} where it is included in the negative sample $M$, the gradient will push the mark embedding to minimize it's norm value.

Empirically, we have found that using a 2-norm in this hazard function performs well in stabilizing and speeding the training process.
